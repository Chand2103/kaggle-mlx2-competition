{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf0e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load both train and test\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "drop_cols = ['id','composition_label_0','composition_label_1','publication_timestamp',\n",
    "             'lunar_phase','creator_collective','composition_label_2','track_identifier']\n",
    "\n",
    "df_train.drop(columns=drop_cols, inplace=True)\n",
    "df_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Separate features and target in train\n",
    "target = df_train['target']\n",
    "df_train.drop(columns=['target'], inplace=True)\n",
    "\n",
    "# Identify numeric and categorical columns from train (important: use train only for this)\n",
    "numeric_cols = df_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Split numeric cols by missingness threshold on train data\n",
    "threshold = int(0.10 * len(df_train))  # 10%\n",
    "numeric_null_counts = df_train[numeric_cols].isna().sum()\n",
    "\n",
    "low_null_numeric = numeric_null_counts[numeric_null_counts <= threshold].index.tolist()\n",
    "high_null_numeric = numeric_null_counts[numeric_null_counts > threshold].index.tolist()\n",
    "\n",
    "# ---------- Numeric imputation ----------\n",
    "# Fit KNN imputer on train low-missing numeric, transform both train and test\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "train_low_num_imputed = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_train[low_null_numeric]),\n",
    "    columns=low_null_numeric\n",
    ")\n",
    "test_low_num_imputed = pd.DataFrame(\n",
    "    knn_imputer.transform(df_test[low_null_numeric]),\n",
    "    columns=low_null_numeric\n",
    ")\n",
    "\n",
    "# Fit SimpleImputer on train high-missing numeric, transform both train and test\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "train_high_num_imputed = pd.DataFrame(\n",
    "    simple_imputer.fit_transform(df_train[high_null_numeric]),\n",
    "    columns=high_null_numeric\n",
    ")\n",
    "test_high_num_imputed = pd.DataFrame(\n",
    "    simple_imputer.transform(df_test[high_null_numeric]),\n",
    "    columns=high_null_numeric\n",
    ")\n",
    "\n",
    "# Combine numeric imputations for train and test\n",
    "train_num_imputed = pd.concat([train_low_num_imputed, train_high_num_imputed], axis=1)\n",
    "test_num_imputed = pd.concat([test_low_num_imputed, test_high_num_imputed], axis=1)\n",
    "\n",
    "# ---------- Categorical imputation ----------\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_cat_imputed = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df_train[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "test_cat_imputed = pd.DataFrame(\n",
    "    cat_imputer.transform(df_test[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# OneHotEncoding\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "train_cat_encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(train_cat_imputed),\n",
    "    columns=encoder.get_feature_names_out(categorical_cols)\n",
    ")\n",
    "test_cat_encoded = pd.DataFrame(\n",
    "    encoder.transform(test_cat_imputed),\n",
    "    columns=encoder.get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "# Combine numeric + categorical features for train and test\n",
    "train_processed = pd.concat([train_num_imputed.reset_index(drop=True), train_cat_encoded.reset_index(drop=True)], axis=1)\n",
    "test_processed = pd.concat([test_num_imputed.reset_index(drop=True), test_cat_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ---------- Scaling ----------\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_processed)\n",
    "test_scaled = scaler.transform(test_processed)\n",
    "\n",
    "\n",
    "\n",
    "# Final DataFrames\n",
    "df_train_final = pd.DataFrame(train_scaled, columns=[f\"PC{i+1}\" for i in range(train_scaled.shape[1])])\n",
    "df_train_final['target'] = target.reset_index(drop=True)\n",
    "\n",
    "df_test_final = pd.DataFrame(test_scaled, columns=[f\"PC{i+1}\" for i in range(test_scaled.shape[1])])\n",
    "\n",
    "# Now df_train_final and df_test_final are ready for modeling!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9bc1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\Desktop\\projects\\tf_project\\tfenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chand\\Desktop\\projects\\tf_project\\tfenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š XGBoost Validation Metrics:\n",
      " - RMSE: 1.7990\n",
      " - MAPE: 0.0697\n",
      " - RÂ²:   0.9931\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Split data for evaluation\n",
    "# -------------------------------\n",
    "X = df_train_final.drop(columns=['target'])\n",
    "y = df_train_final['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# -----------Run only for hyperparameter optimization---------------\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "#         'tree_method': 'gpu_hist'\n",
    "#     }\n",
    "    \n",
    "#     model = XGBRegressor(**params, random_state=42)\n",
    "#     rmse = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "#     return rmse\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Define best hyperparameters and fit model\n",
    "# -------------------------------\n",
    "best_params = {\n",
    "    'n_estimators': 405,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.07748551072473618,\n",
    "    'subsample': 0.8979101171513736,\n",
    "    'colsample_bytree': 0.7371023660083749,\n",
    "    'tree_method': 'gpu_hist',  # use 'gpu_hist' if you're using GPU\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(**best_params)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Evaluate on validation set\n",
    "# -------------------------------\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "val_mape = mean_absolute_percentage_error(y_val, val_preds)\n",
    "val_r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "print(\"\\nðŸ“Š XGBoost Validation Metrics:\")\n",
    "print(f\" - RMSE: {val_rmse:.4f}\")\n",
    "print(f\" - MAPE: {val_mape:.4f}\")\n",
    "print(f\" - RÂ²:   {val_r2:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Predict on test set\n",
    "# -------------------------------\n",
    "test_preds = xgb_model.predict(df_test_final)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Save predictions\n",
    "# -------------------------------\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['target'] = test_preds\n",
    "submission.to_csv(\"xgb_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
